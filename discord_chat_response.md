]0;Kilo Code - financial analyst[2J[3J[H{"timestamp":1,"source":"cli","id":"msg-1765578068988-9331","type":"welcome","content":"","metadata":{"welcomeOptions":{"clearScreen":false,"showInstructions":false,"instructions":[]}}}

[2K[1A[2K[G{"timestamp":1765578069096,"source":"extension","type":"say","say":"text","content":"quel model utilse tu pour inf√©r√©"}

[2K[1A[2K[G{"timestamp":1765578069238,"source":"extension","type":"say","say":"api_req_started","metadata":{"apiProtocol":"openai"}}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The"}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's message is: \"quel model utilse tu pour inf√©r√©"}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's message is: \"quel model utilse tu pour inf√©r√©\"\n"}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's message is: \"quel model utilse tu pour inf√©r√©\"\n\n\n## Interpreting User Query  \n- The user's message, \"quel model utilse tu pour inf√©r√©,\" appears to be in French."}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's message is: \"quel model utilse tu pour inf√©r√©\"\n\n\n## Interpreting User Query  \n- The user's message, \"quel model utilse tu pour inf√©r√©,\" appears to be in French.  \n- It translates to \"What model do you use to infer?\" indicating they're curious about the inference process."}

[2K[1A[2K[G{"timestamp":1765578074395,"source":"extension","type":"say","say":"text","partial":true,"content":"The"}

[2K[1A[2K[G{"timestamp":1765578069238,"source":"extension","type":"say","say":"api_req_started","metadata":{"apiProtocol":"openai","tokensIn":8917,"tokensOut":325,"cacheWrites":0,"cacheReads":4224,"cost":0,"usageMissing":false,"inferenceProvider":"xAI"}}

[2K[1A[2K[G{"timestamp":1765578072117,"source":"extension","type":"say","say":"reasoning","partial":false,"content":"The user's message is: \"quel model utilse tu pour inf√©r√©\"\n\n\n## Interpreting User Query  \n- The user's message, \"quel model utilse tu pour inf√©r√©,\" appears to be in French.  \n- It translates to \"What model do you use to infer?\" indicating they're curious about the inference process."}

[2K[1A[2K[G{"timestamp":1765578074395,"source":"extension","type":"say","say":"text","partial":false,"content":"The model I use for inference is x-ai/grok-code-fast-1."}

[2K[1A[2K[G{"timestamp":1765578074924,"source":"extension","type":"say","say":"api_req_started","metadata":{"apiProtocol":"openai"}}

[2K[1A[2K[G{"timestamp":1765578075945,"event":"session_created","sessionId":"5795805b-1561-4671-9daa-47a67c5c3e8e"}

[2K[1A[2K[G{"sessionId":"5795805b-1561-4671-9daa-47a67c5c3e8e","updatedAt":1765578077616,"timestamp":1765578076677,"event":"session_synced"}

[2K[1A[2K[G{"timestamp":1765578077160,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The"}

[2K[1A[2K[G{"timestamp":1765578077160,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's query is: \"quel model utilse tu pour inf√©r√©"}

[2K[1A[2K[G{"timestamp":1765578077160,"source":"extension","type":"say","say":"reasoning","partial":true,"content":"The user's query is: \"quel model utilse tu pour inf√©r√©\"\n"}

[2K[1A[2K[G{"timestamp":1765578074924,"source":"extension","type":"say","say":"api_req_started","metadata":{"apiProtocol":"openai","tokensIn":9246,"tokensOut":188,"cacheWrites":0,"cacheReads":8896,"cost":0,"usageMissing":false,"inferenceProvider":"xAI"}}

[2K[1A[2K[G{"timestamp":1765578077160,"source":"extension","type":"say","say":"reasoning","partial":false,"content":"The user's query is: \"quel model utilse tu pour inf√©r√©\"\n"}

[2K[1A[2K[G{"timestamp":1765578078474,"source":"extension","type":"say","say":"checkpoint_saved","checkpoint":{"from":"bc3d4b5aeb125317fe7ec566ddc887f5572544e6","to":"e5457a2313328467a795c7c369d848cf7378c724","suppressMessage":false},"content":"e5457a2313328467a795c7c369d848cf7378c724"}

[2K[1A[2K[G{"timestamp":1765578078480,"source":"extension","type":"say","say":"completion_result","partial":true}

[2K[1A[2K[G{"timestamp":1765578078480,"source":"extension","type":"say","say":"completion_result","partial":false,"content":"The model I use for inference is x-ai/grok-code-fast-1."}

[2K[1A[2K[G{"timestamp":1765578078799,"source":"extension","type":"ask","ask":"completion_result"}

[2K[1A[2K[G{"sessionId":"5795805b-1561-4671-9daa-47a67c5c3e8e","updatedAt":1765578080651,"timestamp":1765578079750,"event":"session_synced"}

[2K[1A[2K[G{"sessionId":"5795805b-1561-4671-9daa-47a67c5c3e8e","updatedAt":1765578080809,"timestamp":1765578079877,"event":"session_synced"}

[2K[1A[2K[G{"sessionId":"5795805b-1561-4671-9daa-47a67c5c3e8e","updatedAt":1765578081349,"timestamp":1765578080658,"event":"session_synced"}

[?2004l