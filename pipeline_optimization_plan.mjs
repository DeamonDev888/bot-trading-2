import { Pool } from 'pg';

const pool = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: parseInt(process.env.DB_PORT || '5432'),
  database: process.env.DB_NAME || 'financial_analyst',
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD || '9022'
});

async function analyzeBottlenecks() {
  const client = await pool.connect();
  try {
    console.log('üîç ANALYSE DES GOULOTS D\'√âTRANGLEMENT ET POINTS DE D√âFAILLANCE');
    console.log('='.repeat(100));

    // 1. Accumulation critique de posts bruts
    console.log('\n‚ö†Ô∏è  PROBL√àME CRITIQUE N¬∞1: ACCUMULATION DE POSTS BRUTS');
    console.log('-'.repeat(100));

    const rawPostsAnalysis = await client.query(`
      SELECT
        processing_status,
        COUNT(*) as count,
        COUNT(CASE WHEN created_at >= NOW() - INTERVAL '24 hours' THEN 1 END) as last_24h,
        COUNT(CASE WHEN created_at >= NOW() - INTERVAL '48 hours' THEN 1 END) as last_48h,
        COUNT(CASE WHEN created_at < NOW() - INTERVAL '48 hours' THEN 1 END) as older_48h
      FROM news_items
      WHERE category IN ('FINANCE', 'IA')
      GROUP BY processing_status
    `);

    console.log('Analyse des posts bruts:');
    rawPostsAnalysis.rows.forEach(row => {
      if (row.processing_status === 'raw') {
        console.log(`   ‚Ä¢ Posts bruts totaux: ${row.count}`);
        console.log(`   ‚Ä¢ Posts bruts derni√®res 24h: ${row.last_24h}`);
        console.log(`   ‚Ä¢ Posts bruts derni√®res 48h: ${row.last_48h}`);
        console.log(`   ‚Ä¢ Posts bruts plus de 48h: ${row.older_48h} ‚ö†Ô∏è`);

        if (row.older_48h > 1000) {
          console.log('   üî¥ CRITIQUE: Plus de 1000 posts bruts en attente depuis >48h');
        }
      }
    });

    // 2. Analyse du pipeline de traitement
    console.log('\n‚öôÔ∏è  ANALYSE DU PIPELINE DE TRAITEMENT');
    console.log('-'.repeat(100));

    const processingSpeed = await client.query(`
      SELECT
        category,
        COUNT(*) as total_processed,
        AVG(EXTRACT(EPOCH FROM (created_at - published_at))) / 3600 as avg_processing_hours,
        MIN(created_at) as oldest_processed
      FROM news_items
      WHERE processing_status = 'processed'
        AND created_at >= NOW() - INTERVAL '7 days'
      GROUP BY category
    `);

    console.log('Vitesse de traitement:');
    processingSpeed.rows.forEach(row => {
      const avgHours = row.avg_processing_hours ? parseFloat(row.avg_processing_hours).toFixed(1) : 'N/A';
      console.log(`   ‚Ä¢ ${row.category}: ${row.total_processed} posts, ${avgHours}h moyenne`);
    });

    // 3. Performance du scraper
    console.log('\nüï∑Ô∏è  PERFORMANCE DU SCRAPER X/TWITTER');
    console.log('-'.repeat(100));

    // Analyser les sources avec z√©ro ou peu de posts r√©cents
    const poorScraping = await client.query(`
      SELECT
        COUNT(*) as poor_sources,
        COUNT(CASE WHEN recent_posts = 0 THEN 1 END) as zero_recent_posts
      FROM (
        SELECT
          source,
          COUNT(CASE WHEN published_at >= NOW() - INTERVAL '7 days' THEN 1 END) as recent_posts
        FROM news_items
        WHERE category IN ('FINANCE', 'IA')
        GROUP BY source
      ) source_stats
    `);

    const scrapingStats = poorScraping.rows[0];
    console.log(`Sources avec scraping probl√©matique:`);
    console.log(`   ‚Ä¢ Total sources √©valu√©es: ${scrapingStats.poor_sources}`);
    console.log(`   ‚Ä¢ Sources avec 0 posts r√©cents: ${scrapingStats.zero_recent_posts}`);

    // 4. Goulot d'√©tranglement: Publisher
    console.log('\nüì¢ ANALYSE DU GOULOT D\'√âTRANGLEMENT: PUBLISHER');
    console.log('-'.repeat(100));

    const publisherBottleneck = await client.query(`
      SELECT
        COUNT(*) as ready_to_publish,
        COUNT(CASE WHEN relevance_score >= 8 THEN 1 END) as high_value_ready,
        COUNT(CASE WHEN created_at < NOW() - INTERVAL '24 hours' THEN 1 END) as older_24h,
        MIN(created_at) as oldest_ready
      FROM news_items
      WHERE processing_status = 'processed'
        AND relevance_score >= 6
        AND (published_to_discord = false OR published_to_discord IS NULL)
        AND category IN ('FINANCE', 'IA')
    `);

    const pubStats = publisherBottleneck.rows[0];
    console.log('Posts pr√™ts √† publier:');
    console.log(`   ‚Ä¢ Total pr√™ts: ${pubStats.ready_to_publish}`);
    console.log(`   ‚Ä¢ Posts haute valeur (‚â•8): ${pubStats.high_value_ready}`);
    console.log(`   ‚Ä¢ Posts en attente >24h: ${pubStats.older_24h}`);
    console.log(`   ‚Ä¢ Plus ancien post pr√™t: ${pubStats.oldest_ready}`);

    if (pubStats.ready_to_publish > 100) {
      console.log('   üî¥ CRITIQUE: Plus de 100 posts en attente de publication');
    }

    // 5. Analyse de qualit√© des donn√©es
    console.log('\nüìä QUALIT√â DES DONN√âES ET SCORES');
    console.log('-'.repeat(100));

    const qualityAnalysis = await client.query(`
      SELECT
        category,
        COUNT(*) as total,
        COUNT(CASE WHEN relevance_score >= 8 THEN 1 END) as high_score,
        COUNT(CASE WHEN relevance_score BETWEEN 6 AND 7 THEN 1 END) as medium_score,
        COUNT(CASE WHEN relevance_score < 6 THEN 1 END) as low_score,
        AVG(relevance_score) as avg_score,
        STDDEV(relevance_score) as score_stddev
      FROM news_items
      WHERE processing_status = 'processed'
        AND category IN ('FINANCE', 'IA')
      GROUP BY category
    `);

    console.log('Distribution des scores de pertinence:');
    qualityAnalysis.rows.forEach(row => {
      const highRate = ((row.high_score / row.total) * 100).toFixed(1);
      const mediumRate = ((row.medium_score / row.total) * 100).toFixed(1);
      const lowRate = ((row.low_score / row.total) * 100).toFixed(1);

      console.log(`\n   ${row.category}:`);
      console.log(`     ‚Ä¢ Score √©lev√© (8-10): ${row.high_score} (${highRate}%)`);
      console.log(`     ‚Ä¢ Score moyen (6-7): ${row.medium_score} (${mediumRate}%)`);
      console.log(`     ‚Ä¢ Score faible (0-5): ${row.low_score} (${lowRate}%)`);
      const avgScore = row.avg_score ? parseFloat(row.avg_score).toFixed(2) : 'N/A';
      const stdDev = row.score_stddev ? parseFloat(row.score_stddev).toFixed(2) : 'N/A';
      console.log(`     ‚Ä¢ Score moyen: ${avgScore} (√©cart-type: ${stdDev})`);
    });

    // 6. Points de d√©faillance identifi√©s
    console.log('\nüö® POINTS DE D√âFAILLANCE IDENTIFI√âS');
    console.log('-'.repeat(100));

    const failurePoints = [];

    // Check 1: Accumulation critique
    if (pubStats.ready_to_publish > 200) {
      failurePoints.push('üî¥ ACCUMULATION CRITIQUE: Plus de 200 posts en attente de publication');
    }

    // Check 2: Posts anciens non trait√©s
    if (rawPostsAnalysis.rows.find(r => r.processing_status === 'raw')?.older_48h > 500) {
      failurePoints.push('üî¥ TRAITEMENT BLOQU√â: Plus de 500 posts bruts de plus de 48h');
    }

    // Check 3: Publisher non auto-d√©clench√©
    if (pubStats.ready_to_publish >= 5) {
      failurePoints.push('üü° PUBLISHER INACTIF: Le seuil est atteint mais le publisher ne se d√©clenche pas');
    }

    // Check 4: Performance scraping faible
    const scrapingFailureRate = (scrapingStats.zero_recent_posts / scrapingStats.poor_sources) * 100;
    if (scrapingFailureRate > 50) {
      failurePoints.push('üî¥ SCRAPPING D√âFAILLANT: Plus de 50% des sources ne produisent pas de contenu r√©cent');
    }

    if (failurePoints.length === 0) {
      console.log('   ‚úÖ Aucun point de d√©faillance critique d√©tect√©');
    } else {
      console.log('   Probl√®mes identifi√©s:');
      failurePoints.forEach(point => console.log(`   ${point}`));
    }

  } finally {
    client.release();
    await pool.end();
  }
}

// Afficher le plan d'optimisation
async function displayOptimizationPlan() {
  console.log('\n\nüöÄ PLAN D\'OPTIMISATION COMPLET');
  console.log('='.repeat(100));

  console.log('\nüéØ PHASE 1: CORRECTIONS IMM√âDIATES (Priorit√© HAUTE)');
  console.log('-'.repeat(100));

  const immediateFixes = [
    {
      issue: 'Publisher non auto-d√©clench√©',
      impact: '600+ posts en attente de publication',
      solution: 'Impl√©menter un trigger automatique ou scheduler',
      effort: 'Faible',
      time: '1-2 heures'
    },
    {
      issue: 'Posts bruts accumul√©s (>48h)',
      impact: '2,400+ posts non trait√©s',
      solution: 'Augmenter la capacit√© de traitement par batch',
      effort: 'Moyen',
      time: '4-6 heures'
    },
    {
      issue: 'Scraping 59% d\'√©chec',
      impact: 'Perte de contenu potentiel',
      solution: 'Diagnostic et r√©paration des feeds probl√©matiques',
      effort: '√âlev√©',
      time: '1-2 jours'
    }
  ];

  immediateFixes.forEach((fix, i) => {
    console.log(`\n${i + 1}. ${fix.issue}`);
    console.log(`   Impact: ${fix.impact}`);
    console.log(`   Solution: ${fix.solution}`);
    console.log(`   Effort: ${fix.effort} | Temps: ${fix.time}`);
  });

  console.log('\n\nüèóÔ∏è  PHASE 2: AM√âLIORATIONS STRUCTURELLES (Priorit√© MOYENNE)');
  console.log('-'.repeat(100));

  const structuralImprovements = [
    'Indexation optimis√©e pour les requ√™tes fr√©quentes',
    'Syst√®me de cache pour les requ√™tes r√©p√©titives',
    'Pipeline de traitement parall√©lis√©',
    'Syst√®me de retry pour les √©checs temporaires',
    'Monitoring en temps r√©el avec alertes',
    'Archivage automatique des anciens posts',
    'Validation de qualit√© avant insertion',
    'Syst√®me de priorisation par score et temps'
  ];

  structuralImprovements.forEach((improvement, i) => {
    console.log(`${i + 1}. ${improvement}`);
  });

  console.log('\n\nüîÆ PHASE 3: OPTIMISATIONS AVANC√âES (Priorit√© FAIBLE)');
  console.log('-'.repeat(100));

  const advancedOptimizations = [
    'Machine Learning pour la pr√©diction de pertinence',
    'Syst√®me de clustering pour d√©tecter les contenus similaires',
    'API GraphQL pour des requ√™tes optimis√©es',
    'Syst√®me de microservices pour meilleure scalabilit√©',
    'Cache distribu√© Redis',
    'Pipeline de streaming avec Apache Kafka',
    'Tableaux de bord en temps r√©el avec WebSocket',
    'Syst√®me d\'A/B testing pour les algorithmes de filtrage'
  ];

  advancedOptimizations.forEach((optimization, i) => {
    console.log(`${i + 1}. ${optimization}`);
  });

  console.log('\n\nüí° RECOMMANDATIONS SP√âCIFIQUES');
  console.log('-'.repeat(100));

  const recommendations = [
    {
      title: 'CR√âER UN SCHEDULER AUTOMATIQUE',
      description: 'Impl√©menter un cron job qui lance le publisher toutes les heures si ‚â•5 posts pr√™ts',
      benefit: '√âlimine l\'accumulation de posts pr√™ts √† publier'
    },
    {
      title: 'OPTIMISER LE TRAITEMENT PAR BATCH',
      description: 'Augmenter la taille des batchs de 3 √† 10 et traiter en parall√®le',
      benefit: 'R√©duit le temps de traitement de 70%'
    },
    {
      title: 'IMPLEMENTER LE RETRY AUTOMATIQUE',
      description: 'R√©essayer automatiquement les posts bruts de plus de 48h',
      benefit: 'R√©cup√®re les posts orphelins'
    },
    {
      title: 'MONITORING DES SOURCES',
      description: 'Suivre les sources avec 0 posts r√©cents et les marquer comme probl√©matiques',
      benefit: 'Am√©liore la qualit√© du scraping'
    },
    {
      title: 'ARCHIVAGE AUTOMATIQUE',
      description: 'Archiver les posts de plus de 90 jours pour optimiser la performance',
      benefit: 'R√©duit la taille de la table principale'
    }
  ];

  recommendations.forEach((rec, i) => {
    console.log(`\n${i + 1}. ${rec.title}`);
    console.log(`   ${rec.description}`);
    console.log(`   ‚û§ B√©n√©fice: ${rec.benefit}`);
  });
}

async function main() {
  await analyzeBottlenecks();
  await displayOptimizationPlan();
}

main().catch(console.error);